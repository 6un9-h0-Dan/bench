
Bench documentation
====================
Bench is an open source python library for evaluating generative text models for production use cases. 
The bench evaluation framework is useful for judging and tracking model generations for a specific task, such as:

* Comparing the generations of a free, open source model to the generations of a paid model provider. In this example, we compare news articles summaries of an open source model loaded from huggingface with summaries generated by gpt-3.
  
* Comparing the generations of two different model providers. In this example, we compare the generations of OpenAI and Cohere on a product manual question answering task. 
  
* Comparing the generations of two different global prompt templates. In this example, we compare an out of the box prompt, with an engineered prompt for a python QA task.
  
* Comparing the generations of a new model version with the existing production model.

The main entrypoint for using bench is the :class:`TestSuite <arthur_bench.run.testsuite.TestSuite>`  which is used to create new test suites, initiate test runs, and manage suite and run data.

Get started by visiting our quickstart page[link] or reading about :doc:`key concepts <concepts>`.

Contents
========
.. toctree::
   :maxdepth: 2

   concepts.md
   testsuite
   scoring_methods


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
