{"name": "stack_dist_to_golden", "description": null, "scoring_method": "bertscore", "test_cases": [{"input": "Why am I getting an UnboundLocalError when the variable has a value?", "reference_output": "\n It can be a surprise to get the UnboundLocalError in previously working\ncode when it is modified by adding an assignment statement somewhere in\nthe body of a function. \n This code: \n >>> x = 10\n>>> def bar():\n...     print(x)\n...\n>>> bar()\n10\n\n \n works, but this code: \n >>> x = 10\n>>> def foo():\n...     print(x)\n...     x += 1\n\n \n results in an UnboundLocalError: \n >>> foo()\nTraceback (most recent call last):\n  ...\nUnboundLocalError: local variable 'x' referenced before assignment\n\n \n This is because when you make an assignment to a variable in a scope, that\nvariable becomes local to that scope and shadows any similarly named variable\nin the outer scope.  Since the last statement in foo assigns a new value to\nx, the compiler recognizes it as a local variable.  Consequently when the\nearlier print(x) attempts to print the uninitialized local variable and\nan error results. \n In the example above you can access the outer scope variable by declaring it\nglobal: \n >>> x = 10\n>>> def foobar():\n...     global x\n...     print(x)\n...     x += 1\n...\n>>> foobar()\n10\n\n \n This explicit declaration is required in order to remind you that (unlike the\nsuperficially analogous situation with class and instance variables) you are\nactually modifying the value of the variable in the outer scope: \n >>> print(x)\n11\n\n \n You can do a similar thing in a nested scope using the nonlocal\nkeyword: \n >>> def foo():\n...    x = 10\n...    def bar():\n...        nonlocal x\n...        print(x)\n...        x += 1\n...    bar()\n...    print(x)\n...\n>>> foo()\n10\n11\n\n \n"}, {"input": "What are the rules for local and global variables in Python?", "reference_output": "\n In Python, variables that are only referenced inside a function are implicitly\nglobal.  If a variable is assigned a value anywhere within the function\u2019s body,\nit\u2019s assumed to be a local unless explicitly declared as global. \n Though a bit surprising at first, a moment\u2019s consideration explains this.  On\none hand, requiring global for assigned variables provides a bar\nagainst unintended side-effects.  On the other hand, if global was required\nfor all global references, you\u2019d be using global all the time.  You\u2019d have\nto declare as global every reference to a built-in function or to a component of\nan imported module.  This clutter would defeat the usefulness of the global\ndeclaration for identifying side-effects. \n"}, {"input": "Why do lambdas defined in a loop with different values all return the same result?", "reference_output": "\n Assume you use a for loop to define a few different lambdas (or even plain\nfunctions), e.g.: \n >>> squares = []\n>>> for x in range(5):\n...     squares.append(lambda: x**2)\n\n \n This gives you a list that contains 5 lambdas that calculate x**2.  You\nmight expect that, when called, they would return, respectively, 0, 1,\n4, 9, and 16.  However, when you actually try you will see that\nthey all return 16: \n >>> squares[2]()\n16\n>>> squares[4]()\n16\n\n \n This happens because x is not local to the lambdas, but is defined in\nthe outer scope, and it is accessed when the lambda is called \u2014 not when it\nis defined.  At the end of the loop, the value of x is 4, so all the\nfunctions now return 4**2, i.e. 16.  You can also verify this by\nchanging the value of x and see how the results of the lambdas change: \n >>> x = 8\n>>> squares[2]()\n64\n\n \n In order to avoid this, you need to save the values in variables local to the\nlambdas, so that they don\u2019t rely on the value of the global x: \n >>> squares = []\n>>> for x in range(5):\n...     squares.append(lambda n=x: n**2)\n\n \n Here, n=x creates a new variable n local to the lambda and computed\nwhen the lambda is defined so that it has the same value that x had at\nthat point in the loop.  This means that the value of n will be 0\nin the first lambda, 1 in the second, 2 in the third, and so on.\nTherefore each lambda will now return the correct result: \n >>> squares[2]()\n4\n>>> squares[4]()\n16\n\n \n Note that this behaviour is not peculiar to lambdas, but applies to regular\nfunctions too. \n"}, {"input": "How do I share global variables across modules?", "reference_output": "\n The canonical way to share information across modules within a single program is\nto create a special module (often called config or cfg).  Just import the config\nmodule in all modules of your application; the module then becomes available as\na global name.  Because there is only one instance of each module, any changes\nmade to the module object get reflected everywhere.  For example: \n config.py: \n x = 0   # Default value of the 'x' configuration setting\n\n \n mod.py: \n import config\nconfig.x = 1\n\n \n main.py: \n import config\nimport mod\nprint(config.x)\n\n \n Note that using a module is also the basis for implementing the singleton design\npattern, for the same reason. \n"}, {"input": "What are the \u201cbest practices\u201d for using import in a module?", "reference_output": "\n In general, don\u2019t use from modulename import *.  Doing so clutters the\nimporter\u2019s namespace, and makes it much harder for linters to detect undefined\nnames. \n Import modules at the top of a file.  Doing so makes it clear what other modules\nyour code requires and avoids questions of whether the module name is in scope.\nUsing one import per line makes it easy to add and delete module imports, but\nusing multiple imports per line uses less screen space. \n It\u2019s good practice if you import modules in the following order: \n \nstandard library modules \u2013 e.g. sys, os, argparse, re\nthird-party library modules (anything installed in Python\u2019s site-packages\ndirectory) \u2013 e.g. dateutil, requests, PIL.Image\nlocally developed modules\n \n It is sometimes necessary to move imports to a function or class to avoid\nproblems with circular imports.  Gordon McMillan says: \n \nCircular imports are fine where both modules use the \u201cimport <module>\u201d form\nof import.  They fail when the 2nd module wants to grab a name out of the\nfirst (\u201cfrom module import name\u201d) and the import is at the top level.  That\u2019s\nbecause names in the 1st are not yet available, because the first module is\nbusy importing the 2nd.\n \n In this case, if the second module is only used in one function, then the import\ncan easily be moved into that function.  By the time the import is called, the\nfirst module will have finished initializing, and the second module can do its\nimport. \n It may also be necessary to move imports out of the top level of code if some of\nthe modules are platform-specific.  In that case, it may not even be possible to\nimport all of the modules at the top of the file.  In this case, importing the\ncorrect modules in the corresponding platform-specific code is a good option. \n Only move imports into a local scope, such as inside a function definition, if\nit\u2019s necessary to solve a problem such as avoiding a circular import or are\ntrying to reduce the initialization time of a module.  This technique is\nespecially helpful if many of the imports are unnecessary depending on how the\nprogram executes.  You may also want to move imports into a function if the\nmodules are only ever used in that function.  Note that loading a module the\nfirst time may be expensive because of the one time initialization of the\nmodule, but loading a module multiple times is virtually free, costing only a\ncouple of dictionary lookups.  Even if the module name has gone out of scope,\nthe module is probably available in sys.modules. \n"}, {"input": "Why are default values shared between objects?", "reference_output": "\n This type of bug commonly bites neophyte programmers.  Consider this function: \n def foo(mydict={}):  # Danger: shared reference to one dict for all calls\n    ... compute something ...\n    mydict[key] = value\n    return mydict\n\n \n The first time you call this function, mydict contains a single item.  The\nsecond time, mydict contains two items because when foo() begins\nexecuting, mydict starts out with an item already in it. \n It is often expected that a function call creates new objects for default\nvalues. This is not what happens. Default values are created exactly once, when\nthe function is defined.  If that object is changed, like the dictionary in this\nexample, subsequent calls to the function will refer to this changed object. \n By definition, immutable objects such as numbers, strings, tuples, and None,\nare safe from change. Changes to mutable objects such as dictionaries, lists,\nand class instances can lead to confusion. \n Because of this feature, it is good programming practice to not use mutable\nobjects as default values.  Instead, use None as the default value and\ninside the function, check if the parameter is None and create a new\nlist/dictionary/whatever if it is.  For example, don\u2019t write: \n def foo(mydict={}):\n    ...\n\n \n but: \n def foo(mydict=None):\n    if mydict is None:\n        mydict = {}  # create a new dict for local namespace\n\n \n This feature can be useful.  When you have a function that\u2019s time-consuming to\ncompute, a common technique is to cache the parameters and the resulting value\nof each call to the function, and return the cached value if the same value is\nrequested again.  This is called \u201cmemoizing\u201d, and can be implemented like this: \n # Callers can only provide two parameters and optionally pass _cache by keyword\ndef expensive(arg1, arg2, *, _cache={}):\n    if (arg1, arg2) in _cache:\n        return _cache[(arg1, arg2)]\n\n    # Calculate the value\n    result = ... expensive computation ...\n    _cache[(arg1, arg2)] = result           # Store result in the cache\n    return result\n\n \n You could use a global variable containing a dictionary instead of the default\nvalue; it\u2019s a matter of taste. \n"}, {"input": "How can I pass optional or keyword parameters from one function to another?", "reference_output": "\n Collect the arguments using the * and ** specifiers in the function\u2019s\nparameter list; this gives you the positional arguments as a tuple and the\nkeyword arguments as a dictionary.  You can then pass these arguments when\ncalling another function by using * and **: \n def f(x, *args, **kwargs):\n    ...\n    kwargs['width'] = '14.3c'\n    ...\n    g(x, *args, **kwargs)\n\n \n"}, {"input": "What is the difference between arguments and parameters?", "reference_output": "\n Parameters are defined by the names that appear in a\nfunction definition, whereas arguments are the values\nactually passed to a function when calling it.  Parameters define what\nkind of arguments a function can accept.  For\nexample, given the function definition: \n def func(foo, bar=None, **kwargs):\n    pass\n\n \n foo, bar and kwargs are parameters of func.  However, when calling\nfunc, for example: \n func(42, bar=314, extra=somevar)\n\n \n the values 42, 314, and somevar are arguments. \n"}, {"input": "Why did changing list \u2018y\u2019 also change list \u2018x\u2019?", "reference_output": "\n If you wrote code like: \n >>> x = []\n>>> y = x\n>>> y.append(10)\n>>> y\n[10]\n>>> x\n[10]\n\n \n you might be wondering why appending an element to y changed x too. \n There are two factors that produce this result: \n \nVariables are simply names that refer to objects.  Doing y = x doesn\u2019t\ncreate a copy of the list \u2013 it creates a new variable y that refers to\nthe same object x refers to.  This means that there is only one object\n(the list), and both x and y refer to it.\nLists are mutable, which means that you can change their content.\n \n After the call to append(), the content of the mutable object has\nchanged from [] to [10].  Since both the variables refer to the same\nobject, using either name accesses the modified value [10]. \n If we instead assign an immutable object to x: \n >>> x = 5  # ints are immutable\n>>> y = x\n>>> x = x + 1  # 5 can't be mutated, we are creating a new object here\n>>> x\n6\n>>> y\n5\n\n \n we can see that in this case x and y are not equal anymore.  This is\nbecause integers are immutable, and when we do x = x + 1 we are not\nmutating the int 5 by incrementing its value; instead, we are creating a\nnew object (the int 6) and assigning it to x (that is, changing which\nobject x refers to).  After this assignment we have two objects (the ints\n6 and 5) and two variables that refer to them (x now refers to\n6 but y still refers to 5). \n Some operations (for example y.append(10) and y.sort()) mutate the\nobject, whereas superficially similar operations (for example y = y + [10]\nand sorted(y)) create a new object.  In general in Python (and in all cases\nin the standard library) a method that mutates an object will return None\nto help avoid getting the two types of operations confused.  So if you\nmistakenly write y.sort() thinking it will give you a sorted copy of y,\nyou\u2019ll instead end up with None, which will likely cause your program to\ngenerate an easily diagnosed error. \n However, there is one class of operations where the same operation sometimes\nhas different behaviors with different types:  the augmented assignment\noperators.  For example, += mutates lists but not tuples or ints (a_list\n+= [1, 2, 3] is equivalent to a_list.extend([1, 2, 3]) and mutates\na_list, whereas some_tuple += (1, 2, 3) and some_int += 1 create\nnew objects). \n In other words: \n \nIf we have a mutable object (list, dict, set,\netc.), we can use some specific operations to mutate it and all the variables\nthat refer to it will see the change.\nIf we have an immutable object (str, int, tuple,\netc.), all the variables that refer to it will always see the same value,\nbut operations that transform that value into a new value always return a new\nobject.\n \n If you want to know if two variables refer to the same object or not, you can\nuse the is operator, or the built-in function id(). \n"}, {"input": "How do I write a function with output parameters (call by reference)?", "reference_output": "\n Remember that arguments are passed by assignment in Python.  Since assignment\njust creates references to objects, there\u2019s no alias between an argument name in\nthe caller and callee, and so no call-by-reference per se.  You can achieve the\ndesired effect in a number of ways. \n \nBy returning a tuple of the results:\n>>> def func1(a, b):\n...     a = 'new-value'        # a and b are local names\n...     b = b + 1              # assigned to new objects\n...     return a, b            # return new values\n...\n>>> x, y = 'old-value', 99\n>>> func1(x, y)\n('new-value', 100)\n\n\nThis is almost always the clearest solution.\n\nBy using global variables.  This isn\u2019t thread-safe, and is not recommended.\nBy passing a mutable (changeable in-place) object:\n>>> def func2(a):\n...     a[0] = 'new-value'     # 'a' references a mutable list\n...     a[1] = a[1] + 1        # changes a shared object\n...\n>>> args = ['old-value', 99]\n>>> func2(args)\n>>> args\n['new-value', 100]\n\n\n\nBy passing in a dictionary that gets mutated:\n>>> def func3(args):\n...     args['a'] = 'new-value'     # args is a mutable dictionary\n...     args['b'] = args['b'] + 1   # change it in-place\n...\n>>> args = {'a': 'old-value', 'b': 99}\n>>> func3(args)\n>>> args\n{'a': 'new-value', 'b': 100}\n\n\n\nOr bundle up values in a class instance:\n>>> class Namespace:\n...     def __init__(self, /, **args):\n...         for key, value in args.items():\n...             setattr(self, key, value)\n...\n>>> def func4(args):\n...     args.a = 'new-value'        # args is a mutable Namespace\n...     args.b = args.b + 1         # change object in-place\n...\n>>> args = Namespace(a='old-value', b=99)\n>>> func4(args)\n>>> vars(args)\n{'a': 'new-value', 'b': 100}\n\n\nThere\u2019s almost never a good reason to get this complicated.\n\n \n Your best choice is to return a tuple containing the multiple results. \n"}, {"input": "How do you make a higher order function in Python?", "reference_output": "\n You have two choices: you can use nested scopes or you can use callable objects.\nFor example, suppose you wanted to define linear(a,b) which returns a\nfunction f(x) that computes the value a*x+b.  Using nested scopes: \n def linear(a, b):\n    def result(x):\n        return a * x + b\n    return result\n\n \n Or using a callable object: \n class linear:\n\n    def __init__(self, a, b):\n        self.a, self.b = a, b\n\n    def __call__(self, x):\n        return self.a * x + self.b\n\n \n In both cases, \n taxes = linear(0.3, 2)\n\n \n gives a callable object where taxes(10e6) == 0.3 * 10e6 + 2. \n The callable object approach has the disadvantage that it is a bit slower and\nresults in slightly longer code.  However, note that a collection of callables\ncan share their signature via inheritance: \n class exponential(linear):\n    # __init__ inherited\n    def __call__(self, x):\n        return self.a * (x ** self.b)\n\n \n Object can encapsulate state for several methods: \n class counter:\n\n    value = 0\n\n    def set(self, x):\n        self.value = x\n\n    def up(self):\n        self.value = self.value + 1\n\n    def down(self):\n        self.value = self.value - 1\n\ncount = counter()\ninc, dec, reset = count.up, count.down, count.set\n\n \n Here inc(), dec() and reset() act like functions which share the\nsame counting variable. \n"}, {"input": "How do I copy an object in Python?", "reference_output": "\n In general, try copy.copy() or copy.deepcopy() for the general case.\nNot all objects can be copied, but most can. \n Some objects can be copied more easily.  Dictionaries have a copy()\nmethod: \n newdict = olddict.copy()\n\n \n Sequences can be copied by slicing: \n new_l = l[:]\n\n \n"}, {"input": "How can I find the methods or attributes of an object?", "reference_output": "\n For an instance x of a user-defined class, dir(x) returns an alphabetized\nlist of the names containing the instance attributes and methods and attributes\ndefined by its class. \n"}, {"input": "How can my code discover the name of an object?", "reference_output": "\n Generally speaking, it can\u2019t, because objects don\u2019t really have names.\nEssentially, assignment always binds a name to a value; the same is true of\ndef and class statements, but in that case the value is a\ncallable. Consider the following code: \n >>> class A:\n...     pass\n...\n>>> B = A\n>>> a = B()\n>>> b = a\n>>> print(b)\n<__main__.A object at 0x16D07CC>\n>>> print(a)\n<__main__.A object at 0x16D07CC>\n\n \n Arguably the class has a name: even though it is bound to two names and invoked\nthrough the name B the created instance is still reported as an instance of\nclass A.  However, it is impossible to say whether the instance\u2019s name is a or\nb, since both names are bound to the same value. \n Generally speaking it should not be necessary for your code to \u201cknow the names\u201d\nof particular values. Unless you are deliberately writing introspective\nprograms, this is usually an indication that a change of approach might be\nbeneficial. \n In comp.lang.python, Fredrik Lundh once gave an excellent analogy in answer to\nthis question: \n \nThe same way as you get the name of that cat you found on your porch: the cat\n(object) itself cannot tell you its name, and it doesn\u2019t really care \u2013 so\nthe only way to find out what it\u2019s called is to ask all your neighbours\n(namespaces) if it\u2019s their cat (object)\u2026\n\u2026.and don\u2019t be surprised if you\u2019ll find that it\u2019s known by many names, or\nno name at all!\n \n"}, {"input": "What\u2019s up with the comma operator\u2019s precedence?", "reference_output": "\n Comma is not an operator in Python.  Consider this session: \n >>> \"a\" in \"b\", \"a\"\n(False, 'a')\n\n \n Since the comma is not an operator, but a separator between expressions the\nabove is evaluated as if you had entered: \n (\"a\" in \"b\"), \"a\"\n\n \n not: \n \"a\" in (\"b\", \"a\")\n\n \n The same is true of the various assignment operators (=, += etc).  They\nare not truly operators but syntactic delimiters in assignment statements. \n"}, {"input": "Is there an equivalent of C\u2019s \u201c?:\u201d ternary operator?", "reference_output": "\n Yes, there is. The syntax is as follows: \n [on_true] if [expression] else [on_false]\n\nx, y = 50, 25\nsmall = x if x < y else y\n\n \n Before this syntax was introduced in Python 2.5, a common idiom was to use\nlogical operators: \n [expression] and [on_true] or [on_false]\n\n \n However, this idiom is unsafe, as it can give wrong results when on_true\nhas a false boolean value.  Therefore, it is always better to use\nthe ... if ... else ... form. \n"}, {"input": "Is it possible to write obfuscated one-liners in Python?", "reference_output": "\n Yes.  Usually this is done by nesting lambda within\nlambda.  See the following three examples, slightly adapted from Ulf Bartelt: \n from functools import reduce\n\n# Primes < 1000\nprint(list(filter(None,map(lambda y:y*reduce(lambda x,y:x*y!=0,\nmap(lambda x,y=y:y%x,range(2,int(pow(y,0.5)+1))),1),range(2,1000)))))\n\n# First 10 Fibonacci numbers\nprint(list(map(lambda x,f=lambda x,f:(f(x-1,f)+f(x-2,f)) if x>1 else 1:\nf(x,f), range(10))))\n\n# Mandelbrot set\nprint((lambda Ru,Ro,Iu,Io,IM,Sx,Sy:reduce(lambda x,y:x+'\\n'+y,map(lambda y,\nIu=Iu,Io=Io,Ru=Ru,Ro=Ro,Sy=Sy,L=lambda yc,Iu=Iu,Io=Io,Ru=Ru,Ro=Ro,i=IM,\nSx=Sx,Sy=Sy:reduce(lambda x,y:x+y,map(lambda x,xc=Ru,yc=yc,Ru=Ru,Ro=Ro,\ni=i,Sx=Sx,F=lambda xc,yc,x,y,k,f=lambda xc,yc,x,y,k,f:(k<=0)or (x*x+y*y\n>=4.0) or 1+f(xc,yc,x*x-y*y+xc,2.0*x*y+yc,k-1,f):f(xc,yc,x,y,k,f):chr(\n64+F(Ru+x*(Ro-Ru)/Sx,yc,0,0,i)),range(Sx))):L(Iu+y*(Io-Iu)/Sy),range(Sy\n))))(-2.1, 0.7, -1.2, 1.2, 30, 80, 24))\n#    \\___ ___/  \\___ ___/  |   |   |__ lines on screen\n#        V          V      |   |______ columns on screen\n#        |          |      |__________ maximum of \"iterations\"\n#        |          |_________________ range on y axis\n#        |____________________________ range on x axis\n\n \n Don\u2019t try this at home, kids! \n"}, {"input": "What does the slash(/) in the parameter list of a function mean?", "reference_output": "\n A slash in the argument list of a function denotes that the parameters prior to\nit are positional-only.  Positional-only parameters are the ones without an\nexternally usable name.  Upon calling a function that accepts positional-only\nparameters, arguments are mapped to parameters based solely on their position.\nFor example, divmod() is a function that accepts positional-only\nparameters. Its documentation looks like this: \n >>> help(divmod)\nHelp on built-in function divmod in module builtins:\n\ndivmod(x, y, /)\n    Return the tuple (x//y, x%y).  Invariant: div*y + mod == x.\n\n \n The slash at the end of the parameter list means that both parameters are\npositional-only. Thus, calling divmod() with keyword arguments would lead\nto an error: \n >>> divmod(x=3, y=4)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: divmod() takes no keyword arguments\n\n \n"}, {"input": "How do I specify hexadecimal and octal integers?", "reference_output": "\n To specify an octal digit, precede the octal value with a zero, and then a lower\nor uppercase \u201co\u201d.  For example, to set the variable \u201ca\u201d to the octal value \u201c10\u201d\n(8 in decimal), type: \n >>> a = 0o10\n>>> a\n8\n\n \n Hexadecimal is just as easy.  Simply precede the hexadecimal number with a zero,\nand then a lower or uppercase \u201cx\u201d.  Hexadecimal digits can be specified in lower\nor uppercase.  For example, in the Python interpreter: \n >>> a = 0xa5\n>>> a\n165\n>>> b = 0XB2\n>>> b\n178\n\n \n"}, {"input": "Why does -22 // 10 return -3?", "reference_output": "\n It\u2019s primarily driven by the desire that i % j have the same sign as j.\nIf you want that, and also want: \n i == (i // j) * j + (i % j)\n\n \n then integer division has to return the floor.  C also requires that identity to\nhold, and then compilers that truncate i // j need to make i % j have\nthe same sign as i. \n There are few real use cases for i % j when j is negative.  When j\nis positive, there are many, and in virtually all of them it\u2019s more useful for\ni % j to be >= 0.  If the clock says 10 now, what did it say 200 hours\nago?  -190 % 12 == 2 is useful; -190 % 12 == -10 is a bug waiting to\nbite. \n"}, {"input": "How do I get int literal attribute instead of SyntaxError?", "reference_output": "\n Trying to lookup an int literal attribute in the normal manner gives\na SyntaxError because the period is seen as a decimal point: \n >>> 1.__class__\n  File \"<stdin>\", line 1\n  1.__class__\n   ^\nSyntaxError: invalid decimal literal\n\n \n The solution is to separate the literal from the period\nwith either a space or parentheses. \n >>> 1 .__class__\n<class 'int'>\n>>> (1).__class__\n<class 'int'>\n\n \n"}, {"input": "How do I convert a string to a number?", "reference_output": "\n For integers, use the built-in int() type constructor, e.g. int('144')\n== 144.  Similarly, float() converts to floating-point,\ne.g. float('144') == 144.0. \n By default, these interpret the number as decimal, so that int('0144') ==\n144 holds true, and int('0x144') raises ValueError. int(string,\nbase) takes the base to convert from as a second optional argument, so int(\n'0x144', 16) == 324.  If the base is specified as 0, the number is interpreted\nusing Python\u2019s rules: a leading \u20180o\u2019 indicates octal, and \u20180x\u2019 indicates a hex\nnumber. \n Do not use the built-in function eval() if all you need is to convert\nstrings to numbers.  eval() will be significantly slower and it presents a\nsecurity risk: someone could pass you a Python expression that might have\nunwanted side effects.  For example, someone could pass\n__import__('os').system(\"rm -rf $HOME\") which would erase your home\ndirectory. \n eval() also has the effect of interpreting numbers as Python expressions,\nso that e.g. eval('09') gives a syntax error because Python does not allow\nleading \u20180\u2019 in a decimal number (except \u20180\u2019). \n"}, {"input": "How do I convert a number to a string?", "reference_output": "\n To convert, e.g., the number 144 to the string '144', use the built-in type\nconstructor str().  If you want a hexadecimal or octal representation, use\nthe built-in functions hex() or oct().  For fancy formatting, see\nthe Formatted string literals and Format String Syntax sections,\ne.g. \"{:04d}\".format(144) yields\n'0144' and \"{:.3f}\".format(1.0/3.0) yields '0.333'. \n"}, {"input": "How do I modify a string in place?", "reference_output": "\n You can\u2019t, because strings are immutable.  In most situations, you should\nsimply construct a new string from the various parts you want to assemble\nit from.  However, if you need an object with the ability to modify in-place\nunicode data, try using an io.StringIO object or the array\nmodule: \n >>> import io\n>>> s = \"Hello, world\"\n>>> sio = io.StringIO(s)\n>>> sio.getvalue()\n'Hello, world'\n>>> sio.seek(7)\n7\n>>> sio.write(\"there!\")\n6\n>>> sio.getvalue()\n'Hello, there!'\n\n>>> import array\n>>> a = array.array('u', s)\n>>> print(a)\narray('u', 'Hello, world')\n>>> a[0] = 'y'\n>>> print(a)\narray('u', 'yello, world')\n>>> a.tounicode()\n'yello, world'\n\n \n"}, {"input": "How do I use strings to call functions/methods?", "reference_output": "\n There are various techniques. \n \nThe best is to use a dictionary that maps strings to functions.  The primary\nadvantage of this technique is that the strings do not need to match the names\nof the functions.  This is also the primary technique used to emulate a case\nconstruct:\ndef a():\n    pass\n\ndef b():\n    pass\n\ndispatch = {'go': a, 'stop': b}  # Note lack of parens for funcs\n\ndispatch[get_input()]()  # Note trailing parens to call function\n\n\n\nUse the built-in function getattr():\nimport foo\ngetattr(foo, 'bar')()\n\n\nNote that getattr() works on any object, including classes, class\ninstances, modules, and so on.\nThis is used in several places in the standard library, like this:\nclass Foo:\n    def do_foo(self):\n        ...\n\n    def do_bar(self):\n        ...\n\nf = getattr(foo_instance, 'do_' + opname)\nf()\n\n\n\nUse locals() to resolve the function name:\ndef myFunc():\n    print(\"hello\")\n\nfname = \"myFunc\"\n\nf = locals()[fname]\nf()\n\n\n\n \n"}, {"input": "Is there an equivalent to Perl\u2019s chomp() for removing trailing newlines from strings?", "reference_output": "\n You can use S.rstrip(\"\\r\\n\") to remove all occurrences of any line\nterminator from the end of the string S without removing other trailing\nwhitespace.  If the string S represents more than one line, with several\nempty lines at the end, the line terminators for all the blank lines will\nbe removed: \n >>> lines = (\"line 1 \\r\\n\"\n...          \"\\r\\n\"\n...          \"\\r\\n\")\n>>> lines.rstrip(\"\\n\\r\")\n'line 1 '\n\n \n Since this is typically only desired when reading text one line at a time, using\nS.rstrip() this way works well. \n"}, {"input": "Is there a scanf() or sscanf() equivalent?", "reference_output": "\n Not as such. \n For simple input parsing, the easiest approach is usually to split the line into\nwhitespace-delimited words using the split() method of string objects\nand then convert decimal strings to numeric values using int() or\nfloat().  split() supports an optional \u201csep\u201d parameter which is useful\nif the line uses something other than whitespace as a separator. \n For more complicated input parsing, regular expressions are more powerful\nthan C\u2019s sscanf and better suited for the task. \n"}, {"input": "What does \u2018UnicodeDecodeError\u2019 or \u2018UnicodeEncodeError\u2019 error  mean?", "reference_output": "\n See the Unicode HOWTO. \n"}, {"input": "Can I end a raw string with an odd number of backslashes?", "reference_output": "\n A raw string ending with an odd number of backslashes will escape the string\u2019s quote: \n >>> r'C:\\this\\will\\not\\work\\'\n  File \"<stdin>\", line 1\n    r'C:\\this\\will\\not\\work\\'\n         ^\nSyntaxError: unterminated string literal (detected at line 1)\n\n \n There are several workarounds for this. One is to use regular strings and double\nthe backslashes: \n >>> 'C:\\\\this\\\\will\\\\work\\\\'\n'C:\\\\this\\\\will\\\\work\\\\'\n\n \n Another is to concatenate a regular string containing an escaped backslash to the\nraw string: \n >>> r'C:\\this\\will\\work' '\\\\'\n'C:\\\\this\\\\will\\\\work\\\\'\n\n \n It is also possible to use os.path.join() to append a backslash on Windows: \n >>> os.path.join(r'C:\\this\\will\\work', '')\n'C:\\\\this\\\\will\\\\work\\\\'\n\n \n Note that while a backslash will \u201cescape\u201d a quote for the purposes of\ndetermining where the raw string ends, no escaping occurs when interpreting the\nvalue of the raw string. That is, the backslash remains present in the value of\nthe raw string: \n >>> r'backslash\\'preserved'\n\"backslash\\\\'preserved\"\n\n \n Also see the specification in the language reference. \n"}, {"input": "My program is too slow. How do I speed it up?", "reference_output": "\n That\u2019s a tough one, in general.  First, here are a list of things to\nremember before diving further: \n \nPerformance characteristics vary across Python implementations.  This FAQ\nfocuses on CPython.\nBehaviour can vary across operating systems, especially when talking about\nI/O or multi-threading.\nYou should always find the hot spots in your program before attempting to\noptimize any code (see the profile module).\nWriting benchmark scripts will allow you to iterate quickly when searching\nfor improvements (see the timeit module).\nIt is highly recommended to have good code coverage (through unit testing\nor any other technique) before potentially introducing regressions hidden\nin sophisticated optimizations.\n \n That being said, there are many tricks to speed up Python code.  Here are\nsome general principles which go a long way towards reaching acceptable\nperformance levels: \n \nMaking your algorithms faster (or changing to faster ones) can yield\nmuch larger benefits than trying to sprinkle micro-optimization tricks\nall over your code.\nUse the right data structures.  Study documentation for the Built-in Types\nand the collections module.\nWhen the standard library provides a primitive for doing something, it is\nlikely (although not guaranteed) to be faster than any alternative you\nmay come up with.  This is doubly true for primitives written in C, such\nas builtins and some extension types.  For example, be sure to use\neither the list.sort() built-in method or the related sorted()\nfunction to do sorting (and see the Sorting HOW TO for examples\nof moderately advanced usage).\nAbstractions tend to create indirections and force the interpreter to work\nmore.  If the levels of indirection outweigh the amount of useful work\ndone, your program will be slower.  You should avoid excessive abstraction,\nespecially under the form of tiny functions or methods (which are also often\ndetrimental to readability).\n \n If you have reached the limit of what pure Python can allow, there are tools\nto take you further away.  For example, Cython can\ncompile a slightly modified version of Python code into a C extension, and\ncan be used on many different platforms.  Cython can take advantage of\ncompilation (and optional type annotations) to make your code significantly\nfaster than when interpreted.  If you are confident in your C programming\nskills, you can also write a C extension module\nyourself. \n \nSee also\nThe wiki page devoted to performance tips.\n \n"}, {"input": "What is the most efficient way to concatenate many strings together?", "reference_output": "\n str and bytes objects are immutable, therefore concatenating\nmany strings together is inefficient as each concatenation creates a new\nobject.  In the general case, the total runtime cost is quadratic in the\ntotal string length. \n To accumulate many str objects, the recommended idiom is to place\nthem into a list and call str.join() at the end: \n chunks = []\nfor s in my_strings:\n    chunks.append(s)\nresult = ''.join(chunks)\n\n \n (another reasonably efficient idiom is to use io.StringIO) \n To accumulate many bytes objects, the recommended idiom is to extend\na bytearray object using in-place concatenation (the += operator): \n result = bytearray()\nfor b in my_bytes_objects:\n    result += b\n\n \n"}, {"input": "How do I convert between tuples and lists?", "reference_output": "\n The type constructor tuple(seq) converts any sequence (actually, any\niterable) into a tuple with the same items in the same order. \n For example, tuple([1, 2, 3]) yields (1, 2, 3) and tuple('abc')\nyields ('a', 'b', 'c').  If the argument is a tuple, it does not make a copy\nbut returns the same object, so it is cheap to call tuple() when you\naren\u2019t sure that an object is already a tuple. \n The type constructor list(seq) converts any sequence or iterable into a list\nwith the same items in the same order.  For example, list((1, 2, 3)) yields\n[1, 2, 3] and list('abc') yields ['a', 'b', 'c'].  If the argument\nis a list, it makes a copy just like seq[:] would. \n"}, {"input": "What\u2019s a negative index?", "reference_output": "\n Python sequences are indexed with positive numbers and negative numbers.  For\npositive numbers 0 is the first index 1 is the second index and so forth.  For\nnegative indices -1 is the last index and -2 is the penultimate (next to last)\nindex and so forth.  Think of seq[-n] as the same as seq[len(seq)-n]. \n Using negative indices can be very convenient.  For example S[:-1] is all of\nthe string except for its last character, which is useful for removing the\ntrailing newline from a string. \n"}, {"input": "How do I iterate over a sequence in reverse order?", "reference_output": "\n Use the reversed() built-in function: \n for x in reversed(sequence):\n    ...  # do something with x ...\n\n \n This won\u2019t touch your original sequence, but build a new copy with reversed\norder to iterate over. \n"}, {"input": "How do you remove duplicates from a list?", "reference_output": "\n See the Python Cookbook for a long discussion of many ways to do this: \n \nhttps://code.activestate.com/recipes/52560/\n \n If you don\u2019t mind reordering the list, sort it and then scan from the end of the\nlist, deleting duplicates as you go: \n if mylist:\n    mylist.sort()\n    last = mylist[-1]\n    for i in range(len(mylist)-2, -1, -1):\n        if last == mylist[i]:\n            del mylist[i]\n        else:\n            last = mylist[i]\n\n \n If all elements of the list may be used as set keys (i.e. they are all\nhashable) this is often faster \n mylist = list(set(mylist))\n\n \n This converts the list into a set, thereby removing duplicates, and then back\ninto a list. \n"}, {"input": "How do you remove multiple items from a list", "reference_output": "\n As with removing duplicates, explicitly iterating in reverse with a\ndelete condition is one possibility.  However, it is easier and faster\nto use slice replacement with an implicit or explicit forward iteration.\nHere are three variations.: \n mylist[:] = filter(keep_function, mylist)\nmylist[:] = (x for x in mylist if keep_condition)\nmylist[:] = [x for x in mylist if keep_condition]\n\n \n The list comprehension may be fastest. \n"}, {"input": "How do you make an array in Python?", "reference_output": "\n Use a list: \n [\"this\", 1, \"is\", \"an\", \"array\"]\n\n \n Lists are equivalent to C or Pascal arrays in their time complexity; the primary\ndifference is that a Python list can contain objects of many different types. \n The array module also provides methods for creating arrays of fixed types\nwith compact representations, but they are slower to index than lists.  Also\nnote that NumPy\nand other third party packages define array-like structures with\nvarious characteristics as well. \n To get Lisp-style linked lists, you can emulate cons cells using tuples: \n lisp_list = (\"like\",  (\"this\",  (\"example\", None) ) )\n\n \n If mutability is desired, you could use lists instead of tuples.  Here the\nanalogue of a Lisp car is lisp_list[0] and the analogue of cdr is\nlisp_list[1].  Only do this if you\u2019re sure you really need to, because it\u2019s\nusually a lot slower than using Python lists. \n"}, {"input": "How do I create a multidimensional list?", "reference_output": "\n You probably tried to make a multidimensional array like this: \n >>> A = [[None] * 2] * 3\n\n \n This looks correct if you print it: \n >>> A\n[[None, None], [None, None], [None, None]]\n\n \n But when you assign a value, it shows up in multiple places: \n >>> A[0][0] = 5\n>>> A\n[[5, None], [5, None], [5, None]]\n\n \n The reason is that replicating a list with * doesn\u2019t create copies, it only\ncreates references to the existing objects.  The *3 creates a list\ncontaining 3 references to the same list of length two.  Changes to one row will\nshow in all rows, which is almost certainly not what you want. \n The suggested approach is to create a list of the desired length first and then\nfill in each element with a newly created list: \n A = [None] * 3\nfor i in range(3):\n    A[i] = [None] * 2\n\n \n This generates a list containing 3 different lists of length two.  You can also\nuse a list comprehension: \n w, h = 2, 3\nA = [[None] * w for i in range(h)]\n\n \n Or, you can use an extension that provides a matrix datatype; NumPy is the best known. \n"}, {"input": "How do I apply a method or function to a sequence of objects?", "reference_output": "\n To call a method or function and accumulate the return values is a list,\na list comprehension is an elegant solution: \n result = [obj.method() for obj in mylist]\n\nresult = [function(obj) for obj in mylist]\n\n \n To just run the method or function without saving the return values,\na plain for loop will suffice: \n for obj in mylist:\n    obj.method()\n\nfor obj in mylist:\n    function(obj)\n\n \n"}, {"input": "Why does a_tuple[i] += [\u2018item\u2019] raise an exception when the addition works?", "reference_output": "\n This is because of a combination of the fact that augmented assignment\noperators are assignment operators, and the difference between mutable and\nimmutable objects in Python. \n This discussion applies in general when augmented assignment operators are\napplied to elements of a tuple that point to mutable objects, but we\u2019ll use\na list and += as our exemplar. \n If you wrote: \n >>> a_tuple = (1, 2)\n>>> a_tuple[0] += 1\nTraceback (most recent call last):\n   ...\nTypeError: 'tuple' object does not support item assignment\n\n \n The reason for the exception should be immediately clear: 1 is added to the\nobject a_tuple[0] points to (1), producing the result object, 2,\nbut when we attempt to assign the result of the computation, 2, to element\n0 of the tuple, we get an error because we can\u2019t change what an element of\na tuple points to. \n Under the covers, what this augmented assignment statement is doing is\napproximately this: \n >>> result = a_tuple[0] + 1\n>>> a_tuple[0] = result\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\n\n \n It is the assignment part of the operation that produces the error, since a\ntuple is immutable. \n When you write something like: \n >>> a_tuple = (['foo'], 'bar')\n>>> a_tuple[0] += ['item']\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\n\n \n The exception is a bit more surprising, and even more surprising is the fact\nthat even though there was an error, the append worked: \n >>> a_tuple[0]\n['foo', 'item']\n\n \n To see why this happens, you need to know that (a) if an object implements an\n__iadd__() magic method, it gets called when the += augmented\nassignment\nis executed, and its return value is what gets used in the assignment statement;\nand (b) for lists, __iadd__() is equivalent to calling extend() on the list\nand returning the list.  That\u2019s why we say that for lists, += is a\n\u201cshorthand\u201d for list.extend(): \n >>> a_list = []\n>>> a_list += [1]\n>>> a_list\n[1]\n\n \n This is equivalent to: \n >>> result = a_list.__iadd__([1])\n>>> a_list = result\n\n \n The object pointed to by a_list has been mutated, and the pointer to the\nmutated object is assigned back to a_list.  The end result of the\nassignment is a no-op, since it is a pointer to the same object that a_list\nwas previously pointing to, but the assignment still happens. \n Thus, in our tuple example what is happening is equivalent to: \n >>> result = a_tuple[0].__iadd__(['item'])\n>>> a_tuple[0] = result\nTraceback (most recent call last):\n  ...\nTypeError: 'tuple' object does not support item assignment\n\n \n The __iadd__() succeeds, and thus the list is extended, but even though\nresult points to the same object that a_tuple[0] already points to,\nthat final assignment still results in an error, because tuples are immutable. \n"}, {"input": "I want to do a complicated sort: can you do a Schwartzian Transform in Python?", "reference_output": "\n The technique, attributed to Randal Schwartz of the Perl community, sorts the\nelements of a list by a metric which maps each element to its \u201csort value\u201d. In\nPython, use the key argument for the list.sort() method: \n Isorted = L[:]\nIsorted.sort(key=lambda s: int(s[10:15]))\n\n \n"}, {"input": "How can I sort one list by values from another list?", "reference_output": "\n Merge them into an iterator of tuples, sort the resulting list, and then pick\nout the element you want. \n >>> list1 = [\"what\", \"I'm\", \"sorting\", \"by\"]\n>>> list2 = [\"something\", \"else\", \"to\", \"sort\"]\n>>> pairs = zip(list1, list2)\n>>> pairs = sorted(pairs)\n>>> pairs\n[(\"I'm\", 'else'), ('by', 'sort'), ('sorting', 'to'), ('what', 'something')]\n>>> result = [x[1] for x in pairs]\n>>> result\n['else', 'sort', 'to', 'something']\n\n \n"}, {"input": "What is a class?", "reference_output": "\n A class is the particular object type created by executing a class statement.\nClass objects are used as templates to create instance objects, which embody\nboth the data (attributes) and code (methods) specific to a datatype. \n A class can be based on one or more other classes, called its base class(es). It\nthen inherits the attributes and methods of its base classes. This allows an\nobject model to be successively refined by inheritance.  You might have a\ngeneric Mailbox class that provides basic accessor methods for a mailbox,\nand subclasses such as MboxMailbox, MaildirMailbox, OutlookMailbox\nthat handle various specific mailbox formats. \n"}, {"input": "What is a method?", "reference_output": "\n A method is a function on some object x that you normally call as\nx.name(arguments...).  Methods are defined as functions inside the class\ndefinition: \n class C:\n    def meth(self, arg):\n        return arg * 2 + self.attribute\n\n \n"}, {"input": "What is self?", "reference_output": "\n Self is merely a conventional name for the first argument of a method.  A method\ndefined as meth(self, a, b, c) should be called as x.meth(a, b, c) for\nsome instance x of the class in which the definition occurs; the called\nmethod will think it is called as meth(x, a, b, c). \n See also Why must \u2018self\u2019 be used explicitly in method definitions and calls?. \n"}, {"input": "How do I check if an object is an instance of a given class or of a subclass of it?", "reference_output": "\n Use the built-in function isinstance(obj, cls).  You can\ncheck if an object\nis an instance of any of a number of classes by providing a tuple instead of a\nsingle class, e.g. isinstance(obj, (class1, class2, ...)), and can also\ncheck whether an object is one of Python\u2019s built-in types, e.g.\nisinstance(obj, str) or isinstance(obj, (int, float, complex)). \n Note that isinstance() also checks for virtual inheritance from an\nabstract base class.  So, the test will return True for a\nregistered class even if hasn\u2019t directly or indirectly inherited from it.  To\ntest for \u201ctrue inheritance\u201d, scan the MRO of the class: \n from collections.abc import Mapping\n\nclass P:\n     pass\n\nclass C(P):\n    pass\n\nMapping.register(P)\n\n \n >>> c = C()\n>>> isinstance(c, C)        # direct\nTrue\n>>> isinstance(c, P)        # indirect\nTrue\n>>> isinstance(c, Mapping)  # virtual\nTrue\n\n# Actual inheritance chain\n>>> type(c).__mro__\n(<class 'C'>, <class 'P'>, <class 'object'>)\n\n# Test for \"true inheritance\"\n>>> Mapping in type(c).__mro__\nFalse\n\n \n Note that most programs do not use isinstance() on user-defined classes\nvery often.  If you are developing the classes yourself, a more proper\nobject-oriented style is to define methods on the classes that encapsulate a\nparticular behaviour, instead of checking the object\u2019s class and doing a\ndifferent thing based on what class it is.  For example, if you have a function\nthat does something: \n def search(obj):\n    if isinstance(obj, Mailbox):\n        ...  # code to search a mailbox\n    elif isinstance(obj, Document):\n        ...  # code to search a document\n    elif ...\n\n \n A better approach is to define a search() method on all the classes and just\ncall it: \n class Mailbox:\n    def search(self):\n        ...  # code to search a mailbox\n\nclass Document:\n    def search(self):\n        ...  # code to search a document\n\nobj.search()\n\n \n"}, {"input": "What is delegation?", "reference_output": "\n Delegation is an object oriented technique (also called a design pattern).\nLet\u2019s say you have an object x and want to change the behaviour of just one\nof its methods.  You can create a new class that provides a new implementation\nof the method you\u2019re interested in changing and delegates all other methods to\nthe corresponding method of x. \n Python programmers can easily implement delegation.  For example, the following\nclass implements a class that behaves like a file but converts all written data\nto uppercase: \n class UpperOut:\n\n    def __init__(self, outfile):\n        self._outfile = outfile\n\n    def write(self, s):\n        self._outfile.write(s.upper())\n\n    def __getattr__(self, name):\n        return getattr(self._outfile, name)\n\n \n Here the UpperOut class redefines the write() method to convert the\nargument string to uppercase before calling the underlying\nself._outfile.write() method.  All other methods are delegated to the\nunderlying self._outfile object.  The delegation is accomplished via the\n__getattr__() method; consult the language reference\nfor more information about controlling attribute access. \n Note that for more general cases delegation can get trickier. When attributes\nmust be set as well as retrieved, the class must define a __setattr__()\nmethod too, and it must do so carefully.  The basic implementation of\n__setattr__() is roughly equivalent to the following: \n class X:\n    ...\n    def __setattr__(self, name, value):\n        self.__dict__[name] = value\n    ...\n\n \n Most __setattr__() implementations must modify\nself.__dict__ to store\nlocal state for self without causing an infinite recursion. \n"}, {"input": "How do I call a method defined in a base class from a derived class that extends it?", "reference_output": "\n Use the built-in super() function: \n class Derived(Base):\n    def meth(self):\n        super().meth()  # calls Base.meth\n\n \n In the example, super() will automatically determine the instance from\nwhich it was called (the self value), look up the method resolution\norder (MRO) with type(self).__mro__, and return the next in line after\nDerived in the MRO: Base. \n"}, {"input": "How can I organize my code to make it easier to change the base class?", "reference_output": "\n You could assign the base class to an alias and derive from the alias.  Then all\nyou have to change is the value assigned to the alias.  Incidentally, this trick\nis also handy if you want to decide dynamically (e.g. depending on availability\nof resources) which base class to use.  Example: \n class Base:\n    ...\n\nBaseAlias = Base\n\nclass Derived(BaseAlias):\n    ...\n\n \n"}, {"input": "How do I create static class data and static class methods?", "reference_output": "\n Both static data and static methods (in the sense of C++ or Java) are supported\nin Python. \n For static data, simply define a class attribute.  To assign a new value to the\nattribute, you have to explicitly use the class name in the assignment: \n class C:\n    count = 0   # number of times C.__init__ called\n\n    def __init__(self):\n        C.count = C.count + 1\n\n    def getcount(self):\n        return C.count  # or return self.count\n\n \n c.count also refers to C.count for any c such that isinstance(c,\nC) holds, unless overridden by c itself or by some class on the base-class\nsearch path from c.__class__ back to C. \n Caution: within a method of C, an assignment like self.count = 42 creates a\nnew and unrelated instance named \u201ccount\u201d in self\u2019s own dict.  Rebinding of a\nclass-static data name must always specify the class whether inside a method or\nnot: \n C.count = 314\n\n \n Static methods are possible: \n class C:\n    @staticmethod\n    def static(arg1, arg2, arg3):\n        # No 'self' parameter!\n        ...\n\n \n However, a far more straightforward way to get the effect of a static method is\nvia a simple module-level function: \n def getcount():\n    return C.count\n\n \n If your code is structured so as to define one class (or tightly related class\nhierarchy) per module, this supplies the desired encapsulation. \n"}, {"input": "How can I overload constructors (or methods) in Python?", "reference_output": "\n This answer actually applies to all methods, but the question usually comes up\nfirst in the context of constructors. \n In C++ you\u2019d write \n class C {\n    C() { cout << \"No arguments\\n\"; }\n    C(int i) { cout << \"Argument is \" << i << \"\\n\"; }\n}\n\n \n In Python you have to write a single constructor that catches all cases using\ndefault arguments.  For example: \n class C:\n    def __init__(self, i=None):\n        if i is None:\n            print(\"No arguments\")\n        else:\n            print(\"Argument is\", i)\n\n \n This is not entirely equivalent, but close enough in practice. \n You could also try a variable-length argument list, e.g. \n def __init__(self, *args):\n    ...\n\n \n The same approach works for all method definitions. \n"}, {"input": "I try to use __spam and I get an error about _SomeClassName__spam.", "reference_output": "\n Variable names with double leading underscores are \u201cmangled\u201d to provide a simple\nbut effective way to define class private variables.  Any identifier of the form\n__spam (at least two leading underscores, at most one trailing underscore)\nis textually replaced with _classname__spam, where classname is the\ncurrent class name with any leading underscores stripped. \n This doesn\u2019t guarantee privacy: an outside user can still deliberately access\nthe \u201c_classname__spam\u201d attribute, and private values are visible in the object\u2019s\n__dict__.  Many Python programmers never bother to use private variable\nnames at all. \n"}, {"input": "My class defines __del__ but it is not called when I delete the object.", "reference_output": "\n There are several possible reasons for this. \n The del statement does not necessarily call __del__() \u2013 it simply\ndecrements the object\u2019s reference count, and if this reaches zero\n__del__() is called. \n If your data structures contain circular links (e.g. a tree where each child has\na parent reference and each parent has a list of children) the reference counts\nwill never go back to zero.  Once in a while Python runs an algorithm to detect\nsuch cycles, but the garbage collector might run some time after the last\nreference to your data structure vanishes, so your __del__() method may be\ncalled at an inconvenient and random time. This is inconvenient if you\u2019re trying\nto reproduce a problem. Worse, the order in which object\u2019s __del__()\nmethods are executed is arbitrary.  You can run gc.collect() to force a\ncollection, but there are pathological cases where objects will never be\ncollected. \n Despite the cycle collector, it\u2019s still a good idea to define an explicit\nclose() method on objects to be called whenever you\u2019re done with them.  The\nclose() method can then remove attributes that refer to subobjects.  Don\u2019t\ncall __del__() directly \u2013 __del__() should call close() and\nclose() should make sure that it can be called more than once for the same\nobject. \n Another way to avoid cyclical references is to use the weakref module,\nwhich allows you to point to objects without incrementing their reference count.\nTree data structures, for instance, should use weak references for their parent\nand sibling references (if they need them!). \n Finally, if your __del__() method raises an exception, a warning message\nis printed to sys.stderr. \n"}, {"input": "How do I get a list of all instances of a given class?", "reference_output": "\n Python does not keep track of all instances of a class (or of a built-in type).\nYou can program the class\u2019s constructor to keep track of all instances by\nkeeping a list of weak references to each instance. \n"}, {"input": "Why does the result of id() appear to be not unique?", "reference_output": "\n The id() builtin returns an integer that is guaranteed to be unique during\nthe lifetime of the object.  Since in CPython, this is the object\u2019s memory\naddress, it happens frequently that after an object is deleted from memory, the\nnext freshly created object is allocated at the same position in memory.  This\nis illustrated by this example: \n >>> id(1000) \n13901272\n>>> id(2000) \n13901272\n\n \n The two ids belong to different integer objects that are created before, and\ndeleted immediately after execution of the id() call.  To be sure that\nobjects whose id you want to examine are still alive, create another reference\nto the object: \n >>> a = 1000; b = 2000\n>>> id(a) \n13901272\n>>> id(b) \n13891296\n\n \n"}, {"input": "When can I rely on identity tests with the is operator?", "reference_output": "\n The is operator tests for object identity.  The test a is b is\nequivalent to id(a) == id(b). \n The most important property of an identity test is that an object is always\nidentical to itself, a is a always returns True.  Identity tests are\nusually faster than equality tests.  And unlike equality tests, identity tests\nare guaranteed to return a boolean True or False. \n However, identity tests can only be substituted for equality tests when\nobject identity is assured.  Generally, there are three circumstances where\nidentity is guaranteed: \n 1) Assignments create new names but do not change object identity.  After the\nassignment new = old, it is guaranteed that new is old. \n 2) Putting an object in a container that stores object references does not\nchange object identity.  After the list assignment s[0] = x, it is\nguaranteed that s[0] is x. \n 3) If an object is a singleton, it means that only one instance of that object\ncan exist.  After the assignments a = None and b = None, it is\nguaranteed that a is b because None is a singleton. \n In most other circumstances, identity tests are inadvisable and equality tests\nare preferred.  In particular, identity tests should not be used to check\nconstants such as int and str which aren\u2019t guaranteed to be\nsingletons: \n >>> a = 1000\n>>> b = 500\n>>> c = b + 500\n>>> a is c\nFalse\n\n>>> a = 'Python'\n>>> b = 'Py'\n>>> c = b + 'thon'\n>>> a is c\nFalse\n\n \n Likewise, new instances of mutable containers are never identical: \n >>> a = []\n>>> b = []\n>>> a is b\nFalse\n\n \n In the standard library code, you will see several common patterns for\ncorrectly using identity tests: \n 1) As recommended by PEP 8, an identity test is the preferred way to check\nfor None.  This reads like plain English in code and avoids confusion with\nother objects that may have boolean values that evaluate to false. \n 2) Detecting optional arguments can be tricky when None is a valid input\nvalue.  In those situations, you can create a singleton sentinel object\nguaranteed to be distinct from other objects.  For example, here is how\nto implement a method that behaves like dict.pop(): \n _sentinel = object()\n\ndef pop(self, key, default=_sentinel):\n    if key in self:\n        value = self[key]\n        del self[key]\n        return value\n    if default is _sentinel:\n        raise KeyError(key)\n    return default\n\n \n 3) Container implementations sometimes need to augment equality tests with\nidentity tests.  This prevents the code from being confused by objects such as\nfloat('NaN') that are not equal to themselves. \n For example, here is the implementation of\ncollections.abc.Sequence.__contains__(): \n def __contains__(self, value):\n    for v in self:\n        if v is value or v == value:\n            return True\n    return False\n\n \n"}, {"input": "How can a subclass control what data is stored in an immutable instance?", "reference_output": "\n When subclassing an immutable type, override the __new__() method\ninstead of the __init__() method.  The latter only runs after an\ninstance is created, which is too late to alter data in an immutable\ninstance. \n All of these immutable classes have a different signature than their\nparent class: \n from datetime import date\n\nclass FirstOfMonthDate(date):\n    \"Always choose the first day of the month\"\n    def __new__(cls, year, month, day):\n        return super().__new__(cls, year, month, 1)\n\nclass NamedInt(int):\n    \"Allow text names for some numbers\"\n    xlat = {'zero': 0, 'one': 1, 'ten': 10}\n    def __new__(cls, value):\n        value = cls.xlat.get(value, value)\n        return super().__new__(cls, value)\n\nclass TitleStr(str):\n    \"Convert str to name suitable for a URL path\"\n    def __new__(cls, s):\n        s = s.lower().replace(' ', '-')\n        s = ''.join([c for c in s if c.isalnum() or c == '-'])\n        return super().__new__(cls, s)\n\n \n The classes can be used like this: \n >>> FirstOfMonthDate(2012, 2, 14)\nFirstOfMonthDate(2012, 2, 1)\n>>> NamedInt('ten')\n10\n>>> NamedInt(20)\n20\n>>> TitleStr('Blog: Why Python Rocks')\n'blog-why-python-rocks'\n\n \n"}, {"input": "How do I cache method calls?", "reference_output": "\n The two principal tools for caching methods are\nfunctools.cached_property() and functools.lru_cache().  The\nformer stores results at the instance level and the latter at the class\nlevel. \n The cached_property approach only works with methods that do not take\nany arguments.  It does not create a reference to the instance.  The\ncached method result will be kept only as long as the instance is alive. \n The advantage is that when an instance is no longer used, the cached\nmethod result will be released right away.  The disadvantage is that if\ninstances accumulate, so too will the accumulated method results.  They\ncan grow without bound. \n The lru_cache approach works with methods that have hashable\narguments.  It creates a reference to the instance unless special\nefforts are made to pass in weak references. \n The advantage of the least recently used algorithm is that the cache is\nbounded by the specified maxsize.  The disadvantage is that instances\nare kept alive until they age out of the cache or until the cache is\ncleared. \n This example shows the various techniques: \n class Weather:\n    \"Lookup weather information on a government website\"\n\n    def __init__(self, station_id):\n        self._station_id = station_id\n        # The _station_id is private and immutable\n\n    def current_temperature(self):\n        \"Latest hourly observation\"\n        # Do not cache this because old results\n        # can be out of date.\n\n    @cached_property\n    def location(self):\n        \"Return the longitude/latitude coordinates of the station\"\n        # Result only depends on the station_id\n\n    @lru_cache(maxsize=20)\n    def historic_rainfall(self, date, units='mm'):\n        \"Rainfall on a given date\"\n        # Depends on the station_id, date, and units.\n\n \n The above example assumes that the station_id never changes.  If the\nrelevant instance attributes are mutable, the cached_property approach\ncan\u2019t be made to work because it cannot detect changes to the\nattributes. \n To make the lru_cache approach work when the station_id is mutable,\nthe class needs to define the __eq__() and __hash__()\nmethods so that the cache can detect relevant attribute updates: \n class Weather:\n    \"Example with a mutable station identifier\"\n\n    def __init__(self, station_id):\n        self.station_id = station_id\n\n    def change_station(self, station_id):\n        self.station_id = station_id\n\n    def __eq__(self, other):\n        return self.station_id == other.station_id\n\n    def __hash__(self):\n        return hash(self.station_id)\n\n    @lru_cache(maxsize=20)\n    def historic_rainfall(self, date, units='cm'):\n        'Rainfall on a given date'\n        # Depends on the station_id, date, and units.\n\n \n"}], "created_by": "rowancheung", "bench_version": "0.0.3", "created_at": "2023-06-30T13:35:38.381809"}